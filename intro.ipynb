{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = torch.zeros(5,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n"
     ]
    }
   ],
   "source": [
    "print(z.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 1],\n",
      "        [1, 1]], dtype=torch.int32)\n",
      "torch.int32\n"
     ]
    }
   ],
   "source": [
    "i = torch.ones((2,2),dtype=torch.int)\n",
    "print(i)\n",
    "print(i.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3126, 0.3791],\n",
      "        [0.3087, 0.0736]])\n",
      "tensor([[0.4216, 0.0691],\n",
      "        [0.2332, 0.4047]])\n",
      "tensor([[0.3126, 0.3791],\n",
      "        [0.3087, 0.0736]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1729)\n",
    "\n",
    "t1 = torch.rand(2,2)\n",
    "t2 = torch.rand(2,2)\n",
    "\n",
    "torch.manual_seed(1729)\n",
    "t3 = torch.rand(2,2)\n",
    "\n",
    "print(t1)\n",
    "print(t2)\n",
    "print(t3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Boolean value of Tensor with more than one value is ambiguous",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-949616a8bc59>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mt1\u001b[0m\u001b[1;33m==\u001b[0m\u001b[0mt3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'yes equal'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Boolean value of Tensor with more than one value is ambiguous"
     ]
    }
   ],
   "source": [
    "if (t1==t3):\n",
    "    print('yes equal')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic Mechanics of  a single training pass\n",
    "Autograd -- Automatic Differentiation Engine \n",
    "A model will have forward(), here the actual computation happens\n",
    "\n",
    "\n",
    "# Define Neural network with activation and loss functions\n",
    "\n",
    "class LeNet(nn.Module):\n",
    "      def __init__(self):\n",
    "         super(LeNet,self).__init__()\n",
    "         self.conv1 = nn.Conv2d(1,6,3)\n",
    "         self.fc1   = nn.Linear(84,10)\n",
    "\n",
    "      def forward(self,x):\n",
    "          x = F.max_pool2d(F.relu(self.conv1(x), (2,2)))\n",
    "          x = F.relu(self.fc1(x))\n",
    "\n",
    "\n",
    "# Create Object for the above model classs\n",
    "net = LeNet() #Instatiate class\n",
    "print(net)    \n",
    "\n",
    "# Prepare Input for Inference\n",
    "input = torch.rand(1,1,32,32)  # this tells that this pytorch input has 32*32 image with 1 input channel \n",
    "                               # and the first '1' represents its batch number\n",
    "                               # eg., if the batch number is 16 then input will be (16,1,32,32)\n",
    "                               # pytorch model assumes they are working on batches of data\n",
    "\n",
    "\n",
    "# Inference\n",
    "output = net(input) \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch                       #for all things pytorch\n",
    "import torch.nn as nn              # for torch.nn.Module, the parent object for pytorch Models\n",
    "import torch.nn.functional as F    #for Activation functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data\\cifar-10-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "170499072it [00:19, 8754376.04it/s]                                \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\cifar-10-python.tar.gz to ./data\n"
     ]
    }
   ],
   "source": [
    "# torch.utils.data.Dataset   all downloadable datasets are subclassess of torch.utils.data.Dataset class.\n",
    "# Dataset classes in pytorch include the downloadable datasets in TrochVision, Trochtext and TorchAudio as well as utility dataset classes such as \n",
    "# torchvision.datasets.ImageFolder which reads imagers from folder\n",
    "\n",
    "## Dataset -------> DataLoader \n",
    "\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))])\n",
    "\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data',train=True,download=True, transform = transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Once our dataset is ready we can give it to DataLoader\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size = 4, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "## Putting all pieces together\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn              # For Model\n",
    "import torch.nn.functional as F    # For Activation Function\n",
    "import torch.optim as optim        # For \n",
    "\n",
    "import torchvision                 # For Computer Vision \n",
    "import torchvision.transforms as transforms   # transformation function \n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "#getting data\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))])\n",
    "\n",
    "\n",
    "# Training Data\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data',train=True,download=True, transform = transform)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size = 4, shuffle=True, num_workers=2)\n",
    "\n",
    "#Testing Data\n",
    "testset = torchvision.datasets.CIFAR10(root='./data',train=False,download=True, transform = transform)\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size = 4, shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('plane','car','bird','cat','deer','dog','frog','horse','ship','truck')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net,self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3,6,5)\n",
    "        self.pool  = nn.MaxPool2d(2,2)\n",
    "        self.conv2 = nn.Conv2d(6,16,5)\n",
    "        self.fc1   = nn.Linear(16*5*5,120)\n",
    "        self.fc2   = nn.Linear(120,84)\n",
    "        self.fc3   = nn.Linear(84,10)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1,16 * 5 * 5)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "net = Net()        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(),lr=0.001,momentum=0.9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,  2000] loss:2.179\n",
      "[1,  4000] loss:1.846\n",
      "[1,  6000] loss:1.687\n",
      "[1,  8000] loss:1.608\n",
      "[1, 10000] loss:1.568\n",
      "[1, 12000] loss:1.507\n",
      "[2,  2000] loss:1.448\n",
      "[2,  4000] loss:1.414\n",
      "[2,  6000] loss:1.382\n",
      "[2,  8000] loss:1.370\n",
      "[2, 10000] loss:1.333\n",
      "[2, 12000] loss:1.327\n",
      "finished training\n"
     ]
    }
   ],
   "source": [
    "## Training\n",
    "\n",
    "for epoch in range(2):  # run for 2 epoches\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader,0):\n",
    "        #get the inputs\n",
    "        inputs, labels = data\n",
    "\n",
    "        #zero the parameter gradients\n",
    "        optimizer.zero_grad()   #zero gradients for every batch\n",
    "\n",
    "        #forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs,labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        #print stats\n",
    "        running_loss += loss.item()\n",
    "        if i%2000 == 1999: #print every 2000 mini batches\n",
    "            print('[%d, %5d] loss:%.3f' %(epoch+1, i+1, running_loss / 2000))\n",
    "            running_loss = 0.0 \n",
    "\n",
    "print('finished training')\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy is 52.74\n"
     ]
    }
   ],
   "source": [
    "# Testing\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs.data,1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f\"accuracy is {100 * correct/ total}\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\SUSARSI1\\OneDrive - Novartis Pharma AG\\Desktop\\AIML\\siva-hf\\intro.ipynb Cell 16'\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/SUSARSI1/OneDrive%20-%20Novartis%20Pharma%20AG/Desktop/AIML/siva-hf/intro.ipynb#ch0000015?line=0'>1</a>\u001b[0m \u001b[39m# Deployement with TorchScript\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/SUSARSI1/OneDrive%20-%20Novartis%20Pharma%20AG/Desktop/AIML/siva-hf/intro.ipynb#ch0000015?line=2'>3</a>\u001b[0m my_module \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mjit\u001b[39m.\u001b[39mscript(net(x))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'x' is not defined"
     ]
    }
   ],
   "source": [
    "# Deployement with TorchScript\n",
    "\n",
    "my_module = torch.jit.script(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RecursiveScriptModule(\n",
       "  original_name=Net\n",
       "  (conv1): RecursiveScriptModule(original_name=Conv2d)\n",
       "  (pool): RecursiveScriptModule(original_name=MaxPool2d)\n",
       "  (conv2): RecursiveScriptModule(original_name=Conv2d)\n",
       "  (fc1): RecursiveScriptModule(original_name=Linear)\n",
       "  (fc2): RecursiveScriptModule(original_name=Linear)\n",
       "  (fc3): RecursiveScriptModule(original_name=Linear)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english)\n",
      "c:\\Program Files\\Python39\\lib\\site-packages\\transformers\\pipelines\\token_classification.py:135: UserWarning: `grouped_entities` is deprecated and will be removed in version v5.0.0, defaulted to `aggregation_strategy=\"AggregationStrategy.SIMPLE\"` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'entity_group': 'MISC',\n",
       "  'score': 0.8987888,\n",
       "  'word': 'HuggingFace',\n",
       "  'start': 28,\n",
       "  'end': 39}]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"ner\",grouped_entities=True)\n",
    "classifier(\"I've not been waiting for a HuggingFace course my whole life, this is not going to be work.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'entity_group': 'PER',\n",
       "  'score': 0.9294071,\n",
       "  'word': 'Raghu',\n",
       "  'start': 0,\n",
       "  'end': 5},\n",
       " {'entity_group': 'LOC',\n",
       "  'score': 0.8643474,\n",
       "  'word': 'hyderabad',\n",
       "  'start': 14,\n",
       "  'end': 23},\n",
       " {'entity_group': 'ORG',\n",
       "  'score': 0.9946643,\n",
       "  'word': 'X - RAY Inc',\n",
       "  'start': 46,\n",
       "  'end': 55}]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier(\"Raghu went to hyderabad for dental surgery at X-RAY Inc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to facebook/bart-large-mnli (https://huggingface.co/facebook/bart-large-mnli)\n",
      "Downloading: 100%|██████████| 1.13k/1.13k [00:00<00:00, 230kB/s]\n",
      "Downloading: 100%|██████████| 1.52G/1.52G [00:53<00:00, 30.3MB/s]\n",
      "Downloading: 100%|██████████| 26.0/26.0 [00:00<00:00, 5.20kB/s]\n",
      "Downloading: 100%|██████████| 878k/878k [00:01<00:00, 659kB/s]  \n",
      "Downloading: 100%|██████████| 446k/446k [00:01<00:00, 376kB/s]  \n",
      "Downloading: 100%|██████████| 1.29M/1.29M [00:01<00:00, 971kB/s]\n"
     ]
    }
   ],
   "source": [
    "#Zero Short Classifier\n",
    "\n",
    "from transformers import pipeline\n",
    " \n",
    "zero_classifier = pipeline(\"zero-shot-classification\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sequence': 'Cosentyx marketing campaign',\n",
       " 'labels': ['Public Relations',\n",
       "  'chemicals',\n",
       "  'Secondary Market Research',\n",
       "  'Creative Agency',\n",
       "  'Medical Communications',\n",
       "  'Primary market Research & Competitive Intelligence',\n",
       "  'Health Economics and Access Pricing'],\n",
       " 'scores': [0.5262454152107239,\n",
       "  0.2862624526023865,\n",
       "  0.045808836817741394,\n",
       "  0.0424041822552681,\n",
       "  0.03814248368144035,\n",
       "  0.0307435542345047,\n",
       "  0.030393054708838463]}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zero_classifier(\n",
    "    \"Cosentyx marketing campaign\",\n",
    "    candidate_labels=[ \"Creative Agency\",\"Public Relations\",\"Medical Communications\",\"Primary market Research & Competitive Intelligence\",\n",
    "    \"Secondary Market Research\", \"Health Economics and Access Pricing\",\"chemicals\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Tokenizers and Models Explicitly for Pretrained Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "sequence = \"I've been waiting for a HuggingFace course my whole life.\"\n",
    "\n",
    "model_inputs = tokenizer(sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[101, 1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012, 102], [101, 2061, 2031, 1045, 999, 102]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenizer It also handles multiple sequences at a time, with no change in the API:\n",
    "\n",
    "sequences = [\"I've been waiting for a HuggingFace course my whole life.\", \"So have I!\"]\n",
    "\n",
    "model_inputs = tokenizer(sequences)\n",
    "model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [[101, 1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012, 102], [101, 2061, 2031, 1045, 999, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]}\n",
      "{'input_ids': [[101, 1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012, 102], [101, 2061, 2031, 1045, 999, 102]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1]]}\n",
      "{'input_ids': [[101, 1045, 1005, 2310, 2042, 3403, 2005, 102], [101, 2061, 2031, 1045, 999, 102]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1]]}\n"
     ]
    }
   ],
   "source": [
    "#It can pad according to several objectives:\n",
    "# print(f'max length is {max_length} according to the tokenizer that we use')\n",
    "model_inputs = tokenizer(sequences, padding=\"longest\")\n",
    "# model_inputs = tokenizer(sequences, padding=\"max_length\")\n",
    "# model_inputs = tokenizer(sequences, padding=\"max_length\", max_length=8)\n",
    "\n",
    "print(model_inputs)\n",
    "\n",
    "\n",
    "#It can also truncate sequences:\n",
    "sequences = [\"I've been waiting for a HuggingFace course my whole life.\", \"So have I!\"]\n",
    "# Will truncate the sequences that are longer than the model max length\n",
    "# (512 for BERT or DistilBERT)\n",
    "model_inputs = tokenizer(sequences, truncation=True)\n",
    "print(model_inputs)\n",
    "\n",
    "# Will truncate the sequences that are longer than the specified max length\n",
    "model_inputs = tokenizer(sequences, max_length=8, truncation=True)\n",
    "print(model_inputs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', \"'\", 've', 'been', 'waiting', 'for', 'a', 'hugging', '##face', 'course', 'my', 'whole', 'life', '.', 'so', 'have', 'i', '!']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.tokenize(sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "11938c6bc6919ae2720b4d5011047913343b08a43b18698fd82dedb0d4417594"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
